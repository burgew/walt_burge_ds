{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Exploration: Toxic Language Classification \n",
    "**w207 Spring 2018 - Final Project Baseline**\n",
    "\n",
    "**Team: Paul, Walt, Yisang, Joe**\n",
    "\n",
    "\n",
    "\n",
    "## Project Description \n",
    "\n",
    "Our challenge is to build a multi-headed model thatâ€™s capable of detecting different types of of toxicity like threats, obscenity, insults, and identity-based hate.  The toxic language data set is sourced from Wikipedia and available as a public kaggle data set. \n",
    "\n",
    "Our goal is to use various machine learning techniques used in class to develop high quality ML models and pipelines.  \n",
    "\n",
    "Our research deliverables will be the following:\n",
    "1. Exercise and build upon concepts covered in class and test out the following supervised models:\n",
    "    a. Regression (LASSO, Logistic)\n",
    "    b. Naive Bayes\n",
    "    c. Trees (XGBoost)\n",
    "    d. Neural Networks MPI\n",
    "    c. KNN\n",
    "2. Using stacking/ensembling methods (simple blending)\n",
    "\n",
    "\n",
    "For the baseline proposal, this file contains a first pass run through from data preprocessing to model evaluation using a regression model pipeline. \n",
    "\n",
    "https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge\n",
    "\n",
    "\n",
    "\n",
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joseph Lee\\AppData\\Local\\conda\\conda\\envs\\mids\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\Joseph Lee\\AppData\\Local\\conda\\conda\\envs\\mids\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#sklearn imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#scipy imports\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "#Visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec \n",
    "import bokeh\n",
    "#! pip install bokeh\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "import string\n",
    "from sklearn import metrics\n",
    "import ast\n",
    "\n",
    "# target classes\n",
    "target_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('total training observations:', 111828)\n",
      "('training data shape:', (111828L,))\n",
      "('training label shape:', (111828, 6))\n",
      "('dev label shape:', (47743, 6))\n",
      "('labels names:', ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'])\n"
     ]
    }
   ],
   "source": [
    "# read frames localy through csv\n",
    "train_df = pd.read_csv(\"../data/new_train.csv\")\n",
    "test_df = pd.read_csv(\"../data/new_test.csv\")\n",
    "\n",
    "# # Random index generator for splitting training data\n",
    "# # Note: Each rerun of cell will create new splits.\n",
    "# randIndexCut = np.random.rand(len(train_df)) < 0.7\n",
    "\n",
    "# #S plit up data\n",
    "# test_data = test_df[\"comment_text\"]\n",
    "# dev_data, dev_labels = train_df[~randIndexCut][\"comment_text\"], train_df[~randIndexCut][target_names]\n",
    "# train_data, train_labels = train_df[randIndexCut][\"comment_text\"], train_df[randIndexCut][target_names]\n",
    "\n",
    "dev_data, dev_labels = test_df[\"comment_text\"], test_df[target_names]\n",
    "train_data, train_labels = train_df[\"comment_text\"], train_df[target_names]\n",
    "\n",
    "print('total training observations:', train_df.shape[0])\n",
    "print('training data shape:', train_data.shape)\n",
    "print('training label shape:', train_labels.shape)\n",
    "print('dev label shape:', dev_labels.shape)\n",
    "print('labels names:', target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Pre-Processing\n",
    "\n",
    "The following cell is the text procesing used for the final NB model. \n",
    "Many of the functions were drawn from Walt's Neural Network code found here. \n",
    "\n",
    "Future improvements would be to pipe in the Walt's Neural Network classes as a package to minimize code redundancies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Courtesy of Walt\n",
    "\n",
    "import nltk\n",
    "# These imports enable the use of NLTKPreprocessor in an sklearn Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import punkt as punkt\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "\n",
    "class NLTKPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Text preprocessor using NLTK tokenization and Lemmatization\n",
    "\n",
    "    This class is to be used in an sklean Pipeline, prior to other processers like PCA/LSA/classification\n",
    "    Attributes:\n",
    "        lower: A boolean indicating whether text should be lowercased by preprocessor\n",
    "                default: True\n",
    "        strip: A boolean indicating whether text should be stripped of surrounding whitespace, underscores and '*'\n",
    "                default: True\n",
    "        stopwords: A set of words to be used as stop words and thus ignored during tokenization\n",
    "                default: built-in English stop words\n",
    "        punct: A set of punctuation characters that should be ignored\n",
    "                default: None\n",
    "        lemmatizer: An object that should be used to lemmatize tokens\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stopwords=None, punct=None,\n",
    "                 lower=True, strip=True):\n",
    "        self.lower      = lower\n",
    "        self.strip      = strip\n",
    "        self.stopwords  = stopwords or set(sw.words('english'))\n",
    "        self.punct      = punct or set(string.punctuation)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        return [\" \".join(doc) for doc in X]\n",
    "\n",
    "    def transform(self, X):\n",
    "        return [\n",
    "            list(self.tokenize(doc)) for doc in X\n",
    "        ]\n",
    "\n",
    "    def tokenize(self, document):\n",
    "\n",
    "        # Break the document into sentences\n",
    "        for sent in sent_tokenize(unicode(document, 'utf8')):\n",
    "\n",
    "            # Break the sentence into part of speech tagged tokens\n",
    "            for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "                # Apply preprocessing to the token\n",
    "                token = token.lower() if self.lower else token\n",
    "                token = token.strip() if self.strip else token\n",
    "                token = token.strip('_') if self.strip else token\n",
    "                token = token.strip('*') if self.strip else token\n",
    "\n",
    "                # If stopword, ignore token and continue\n",
    "                if token in self.stopwords:\n",
    "                    continue\n",
    "\n",
    "                # If punctuation, ignore token and continue\n",
    "                if all(char in self.punct for char in token):\n",
    "                    continue\n",
    "\n",
    "                # Lemmatize the token and yield\n",
    "                lemma = self.lemmatize(token, tag)\n",
    "                \n",
    "                # S\n",
    "                yield lemma\n",
    "\n",
    "    def lemmatize(self, token, tag):\n",
    "        tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(tag[0], wn.NOUN)\n",
    "\n",
    "        return self.lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "def identity(arg):\n",
    "    \"\"\"\n",
    "    Simple identity function works as a passthrough.\n",
    "    \"\"\"\n",
    "    return arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of scores on dev set and training set\n",
    "def score_classifier_train_on_dev(dev_vector, train_vector, dev_labels, train_labels, label, ctype, pscoring):\n",
    "    \"\"\"This function takes two vectors, one for training and one for dev, trains them\n",
    "    on the selected Naive Bayes model, then depending on the scoring required it\n",
    "    finds the optimal alpha for the particular scoring and calculates that score from\n",
    "    predictions on the dev set.\n",
    "    \n",
    "    Args:\n",
    "        dev_vector: the processed vector of dev data\n",
    "        train_vector: the processed vector of training data\n",
    "        dev_labels: the vector of each of the 6 lables for the dev set\n",
    "        train_labels: the vector of labels for the training set\n",
    "        label (string) : the label name to test\n",
    "        ctype: multi, gaus or bern, choses between multinomial, gaussian or bernoulli Naive Bayes\n",
    "        scoring: should be one of roc_auc, precision, or recall\n",
    "        \n",
    "    Returns:\n",
    "        alpha: the best alpha value for this classifier\n",
    "        score: the score when using this classifier to predict dev\n",
    "    \"\"\"\n",
    "    alphas = {'alpha': [0.0, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0, 15.0, 20.0, 50.0, 100.0]}\n",
    "\n",
    "    if pscoring != 'precision' and pscoring != 'recall' and pscoring != 'roc_auc' and pscoring != 'f1':\n",
    "        print('score_classifier_train_on_dev: Invalid input parameter %s' %(pscoring))\n",
    "        return\n",
    "    \n",
    "    if ctype == 'multi':\n",
    "        nb_class = MultinomialNB().fit(train_vector, train_labels[label])\n",
    "    elif ctype == 'bern':\n",
    "        nb_class = BernoulliNB().fit(train_vector, train_labels[label])\n",
    "    elif ctype == 'gaus':\n",
    "        nb_class = GaussianNB().fit(train_vector, train_labels[label])\n",
    "    else:\n",
    "        print('ctype = %s, error' % (ctype))\n",
    "        return\n",
    "    \n",
    "    # use this to generate the best fitting model for AUC scoring\n",
    "    clf = GridSearchCV(nb_class, param_grid = alphas, scoring=pscoring)\n",
    "    clf.fit(train_vector, train_labels[label])\n",
    "    \n",
    "    # Predict the dev vector\n",
    "    predicted_labels_dev = clf.predict(dev_vector)\n",
    "    \n",
    "    rscore = 0 # return score\n",
    "    # now calculate the score of interested based on the function parameter pscoring\n",
    "    if pscoring == 'precision':\n",
    "        rscore = metrics.precision_score(dev_labels[label], predicted_labels_dev)\n",
    "    elif pscoring == 'recall':\n",
    "        rscore = metrics.recall_score(dev_labels[label], predicted_labels_dev)\n",
    "    elif pscoring == 'f1':\n",
    "        rscore = metrics.f1_score(dev_labels[label], predicted_labels_dev)\n",
    "    else:\n",
    "        rscore = metrics.roc_auc_score(dev_labels[label], predicted_labels_dev)\n",
    "\n",
    "    return clf.best_params_, rscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_all_vectors(my_feature_sizes = [None, 3000, 4000, 5000, 6000, 10000],\n",
    "                       my_stop_words = [None, 'english'],\n",
    "                       my_strip_accents = [None, 'ascii', 'unicode'],\n",
    "                       my_lowercase = [True, False],\n",
    "                       basetrain_data=[],\n",
    "                       basedev_data=[],\n",
    "                       preprocessedtrain_data=[],\n",
    "                       preprocesseddev_data=[],\n",
    "                       verbose=False):\n",
    "    \"\"\"This loops through the lists in the parameters creating 2 vector sets for each combination, \n",
    "    one CountVectorizer and one for TfidfVectorizer.  It allows for both preprocessed and unprocessed\n",
    "    input data, and in the case of pre-processed it does not use the options to strip_accents or\n",
    "    lowercase the data, those options are assumed to have occurred when the data was preprocessed.\n",
    "    \n",
    "    Args:\n",
    "        my_feature_size (list of sizes): Non-empty list of feature sizes to use in vectors\n",
    "        my_stop_words (list of stop_words): Provided to the vectorizers\n",
    "        my_strip_accents (list of options): Provided to the vectorizers\n",
    "        my_lowercase (bool): Provided to the vectorizer\n",
    "        basetrain_data (Opt: list of input data): this is the base training data, no preprocessing\n",
    "        basedev_data (Opt: list of input data): this is the base dev data, no preprocessing\n",
    "        preprocessedtrain_data (Opt: list of input data): this is the base training data that received preprocessing\n",
    "        preprocesseddev_data= (Opt: list of input data): this is the base dev data that received preprocessing\n",
    "        verbose (bool): to write progress outputs\n",
    "        \n",
    "    Returns: \n",
    "        vectors_all (Pandas Datafram) : a dataframe where each line contains the unique count or tfidf vector\n",
    "            along with the set of parameters that were used to create it.\n",
    "    \"\"\"\n",
    "    \n",
    "    vectors_all=pd.DataFrame(columns=['vectortrain', 'vectordev','type','preprocessor', 'tokenizer',\n",
    "                                      'max_features', 'stop_words', 'lowercase', 'strip_accents' ])\n",
    "\n",
    "    index=1\n",
    "    if len(basetrain_data) != 0 and len(basedev_data) != 0:\n",
    "        # we have unprocessed data so create vectors for it\n",
    "        for i in my_feature_sizes:\n",
    "            for x in my_stop_words:\n",
    "                for y in my_strip_accents:\n",
    "                    for z in my_lowercase:\n",
    "                        if (verbose == True):\n",
    "                            print(\"%s: Processing the next vector from base data, index %d\" % (str(datetime.datetime.now().time()),index))\n",
    "                            index +=1\n",
    "#                        ### Count Vectorizer removed as the results are identical (to millions of a %) for count and tfidf\n",
    "#                        ### vectorizer.  We stick with just tfidf as this works best for the other models\n",
    "#                        # Create a count vectorizer with the provided parameters\n",
    "#                        vect = CountVectorizer(max_features=i, stop_words=x, strip_accents=y, lowercase=z)\n",
    "#                        # Train the unpreprocessed training set\n",
    "#                        vect_train = vect.fit_transform(train_data)\n",
    "#                        # Transform the dev set for fuuture predictions\n",
    "#                        vect_dev = vect.transform(dev_data)\n",
    "#                        # add into the output data frame with the list of vectors chosen\n",
    "#                        vectors_all.loc[vectors_all.shape[0]] = [vect_train, vect_dev, 'count', 0, 0, i, x, z, y]\n",
    "\n",
    "                        # Now create tfidf vectorizer for the set of parameters\n",
    "                        vect = TfidfVectorizer(max_features=i, stop_words=x, strip_accents=y, analyzer='word',lowercase=z)\n",
    "                        # Train the unpreprocessed training set\n",
    "                        vect_train = vect.fit_transform(train_data)\n",
    "                        # Transform the dev set for fuuture predictions\n",
    "                        vect_dev = vect.transform(dev_data)\n",
    "                        # add into the output data frame with the list of vectors chosen\n",
    "                        vectors_all.loc[vectors_all.shape[0]] = [vect_train, vect_dev, 'tfidf', 0, 0, i, x, z, y]\n",
    "\n",
    "    index=1\n",
    "    if len(preprocessedtrain_data) != 0 and len(preprocesseddev_data) != 0:\n",
    "        # Separate loop for the preprocessed data as we cannot set the lowercase or strip accents parameters on these\n",
    "        for i in my_feature_sizes:\n",
    "            for x in my_stop_words:\n",
    "                print(\"%s: Processing the next vector from preprocessed data, index %d\" % (str(datetime.datetime.now().time()),index))\n",
    "                index +=1\n",
    "#               ### Count Vectorizer removed as the results are identical (to millions of a %) for count and tfidf\n",
    "#               ### vectorizer.  We stick with just tfidf as this works best for the other models\n",
    "#                # Same but with the preprocessed data\n",
    "#                vect = CountVectorizer(tokenizer=identity, max_features=i, stop_words=x,strip_accents=None, lowercase=False)\n",
    "#                vect_train= vect.fit_transform(train_preproc_data)\n",
    "#                vect_dev= vect.transform(dev_preproc_data)\n",
    "#                vectors_all.loc[vectors_all.shape[0]] = [vect_train, vect_dev, 'count', 1, 0, i, x, False, None]\n",
    "\n",
    "                # Create a tfidf but fit with the preprocessed data\n",
    "                vect = TfidfVectorizer(tokenizer=identity,max_features=i, stop_words=x,strip_accents=None, lowercase=False)\n",
    "                vect_train = vect.fit_transform(train_preproc_data)\n",
    "                vect_dev = vect.transform(dev_preproc_data)\n",
    "                vectors_all.loc[vectors_all.shape[0]] = [vect_train, vect_dev, 'tfidf', 1, 0, i, x, False, None]\n",
    "\n",
    "    if verbose == True:\n",
    "        print('%s: Completed create_all_vectors' % (str(datetime.datetime.now().time())))\n",
    "        \n",
    "    return vectors_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "The code below contains the wrapper functions that trains a variety of Naive Bayes models and report the performance metrics that were established in the main notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score_all_models (vectors_all, score_types = ['precision', 'recall', 'roc_auc'], \n",
    "                                model_types = ['multi', 'bern'], verbose=False):\n",
    "    \"\"\"This function takes a vector of type vectors_all (defined above) acts as a wrapper\n",
    "    to send each vector to the score_classifier_train_on_dev function for scoring.  The\n",
    "    resulting scores are stored in a dataframe and returned\n",
    "    \n",
    "    Args:\n",
    "        vectors_all (dataframe) : a dataframe defined above that stores the vector data in each row\n",
    "        score_types (list) : a list of scoring types to be passed to the scoring\n",
    "        model_types (list) : a list of the Naive Bayes model types to create when scoring these vectors\n",
    "        verbose (bool): print out progress when true\n",
    "    Returns\n",
    "        dataframe: A dataframe of all the resulting scores and the details for each model\n",
    "    \"\"\"\n",
    "    data_all=pd.DataFrame(columns=['vectorno', 'label', 'model','alpha', 'type', 'preprocessor', 'tokenizer', \n",
    "                                   'max_features', 'stop_words', 'lowercase', 'strip_accents',\n",
    "                                   'score_type', 'score'])\n",
    "#     score_types = ['precision', 'recall', 'roc_auc']\n",
    "#     model_types = ['multi', 'bern']\n",
    "    \n",
    "    if verbose == True:\n",
    "        print('%s: Starting calculate_score_all_models' % (str(datetime.datetime.now().time())))        \n",
    "    for index,row in vectors_all.iterrows():\n",
    "        if verbose == True:\n",
    "            print('%s: checking row %d' % (str(datetime.datetime.now().time()),index))\n",
    "        for name in target_names:\n",
    "            for score_type in score_types:\n",
    "                for model_type in model_types:\n",
    "                    # Calculate the score for this pair of vectors with a variety of scoring\n",
    "                    # parameters and types of NB classifier\n",
    "                    alpha, score = score_classifier_train_on_dev(train_vector=row['vectortrain'], \n",
    "                                        dev_vector=row['vectordev'], dev_labels=dev_labels,\n",
    "                                        train_labels=train_labels, label=name, ctype=model_type, pscoring=score_type )\n",
    "                    \n",
    "                    # Store all the results in the dataframe\n",
    "                    data_all.loc[data_all.shape[0]] = [index,name,model_type,alpha,row['type'], \n",
    "                                        row['preprocessor'], row['tokenizer'], row['max_features'], \n",
    "                                        row['stop_words'], row['lowercase'], row['strip_accents'],\n",
    "                                        score_type, score]\n",
    "    if verbose == True:\n",
    "        print('%s: finished calculate_score_all_models' % (str(datetime.datetime.now().time())))               \n",
    "    return data_all\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_out_predictions(vector_all_vectors,vectors_to_predict, output_file, verbose=True):\n",
    "    \"\"\"This function takes a list of all the vectors and a subset of results.  Using the subset\n",
    "    it extracts the training and dev vectors then creates the models, trains them and writes the predictions\n",
    "    to the output file.  This does a prediction of each label and writes them out to the file.\n",
    "    \n",
    "    Args:\n",
    "        vector_all_vectors (dataframe of vector information): This is the dataframe used to store the \n",
    "            vectors.  It has the following fields:\n",
    "                ['vectortrain', 'vectordev','type','preprocessor', 'tokenizer',\n",
    "                    'max_features', 'stop_words', 'lowercase', 'strip_accents' ]\n",
    "        vectors_to_predict (dataframe of results): This is hte dataframe used to store parameters and\n",
    "            results.  It has the following fields:\n",
    "                ['vectorno', 'label', 'model','alpha', 'type', 'preprocessor', 'tokenizer', \n",
    "                    'max_features', 'stop_words', 'lowercase', 'strip_accents',\n",
    "                    'score_type', 'score'])\n",
    "        output_file (string): output file name\n",
    "        verbose (bool): extra printing of information\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    \n",
    "    df_store = pd.DataFrame()\n",
    "    if verbose == True:\n",
    "        print(\"%s: Starting write_out_predictions to %s\" % (str(datetime.datetime.now().time()),output_file))\n",
    "    for index,row in vectors_to_predict.iterrows():\n",
    "        stored_vector_entry = vector_all_vectors.loc[row['vectorno']]\n",
    "        if row['model'] == 'multi':\n",
    "            nb_class = MultinomialNB(alpha=row['alpha'].get('alpha')).fit(stored_vector_entry['vectortrain'], train_labels['toxic'])\n",
    "        elif row['model'] == 'bern':\n",
    "            nb_class = BernoulliNB(alpha=row['alpha'].get('alpha')).fit(stored_vector_entry['vectortrain'], train_labels[row['label']])\n",
    "        else:\n",
    "            print('%s: Error, row is %s' % (str(datetime.datetime.now().time()),row['model'] ))\n",
    "        result_tmp=nb_class.predict(stored_vector_entry['vectordev'])\n",
    "        df_store[row['label']] = result_tmp\n",
    "    if verbose == True:\n",
    "        print(\"%s: Predictions done, writing out\" % (str(datetime.datetime.now().time())))\n",
    "    df_store.to_csv(output_file, index=False)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_out_results(result_df):\n",
    "    \"\"\" This is a wrapper function around the writing out of results.  It writes out a large number\n",
    "    of csv and parm files.  These files are the prediction files and the parameters for each of the\n",
    "    classifiers in the predictions.  The idea here is to write out the following sets of results\n",
    "    1) Absolute best scores\n",
    "    2) Best scores with TFIDF (once I've removed the CountVectorizer this will be the same as 1)\n",
    "    3) Top scores for the NTLK preprocessed data\n",
    "    4) Top scores for TFIDF sorted by feature sizes\n",
    "    5) Top scores for TFIDF with NLTK preprocessing sorted by feature size\n",
    "    With each of these output sets we also write out a parm file which can be rearead later using \n",
    "    the function read_create_classifiers to recreate the vectors and classifiers to match these results.\n",
    "    \n",
    "    Args: \n",
    "        result_df (dataframe of results)\n",
    "    \n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "\n",
    "    score_types = ['precision', 'recall', 'roc_auc']\n",
    "\n",
    "    top_score_results = pd.DataFrame(columns=['vectorno', 'label', 'model','alpha', 'type', 'preprocessor', 'tokenizer', \n",
    "                                       'max_features', 'stop_words', 'lowercase', 'strip_accents',\n",
    "                                       'score_type', 'score'])\n",
    "    filename_prefix = 'NB_predictions_'\n",
    "    # Top scores irrespective\n",
    "    for score_type in score_types:\n",
    "        top_score_results_tmp = top_score_results[0:0]  # empty dataframe in each loop\n",
    "        for label in target_names:\n",
    "            df_tmp = result_df[(result_df['label'] == label) & (result_df['score_type'] == score_type)]\n",
    "            top_score_results_tmp.loc[top_score_results_tmp.shape[0]] = df_tmp.loc[df_tmp['score'].idxmax()]\n",
    "        filename_csv = filename_prefix + 'top_scores_nofilter_' + score_type + '.csv'\n",
    "        filename_parm = filename_prefix + 'top_scores_nofilter_' + score_type + '.parm'\n",
    "        write_out_predictions(vectors_all,top_score_results_tmp, filename_csv, verbose)\n",
    "        top_score_results_tmp.to_csv(filename_parm, index=False)\n",
    "        print(\"Output written to %s\" %(filename_csv))\n",
    "        top_score_results_tmp\n",
    "\n",
    "    # Top Scores for TFIDF - Should be identical\n",
    "    for score_type in score_types:\n",
    "        top_score_results_tmp = top_score_results[0:0]  # empty dataframe in each loop\n",
    "        for label in target_names:\n",
    "            df_tmp = result_df[(result_df['label'] == label) & (result_df['score_type'] == score_type) &\n",
    "                              (result_df['type'] == 'tfidf')]\n",
    "            top_score_results_tmp.loc[top_score_results_tmp.shape[0]] = df_tmp.loc[df_tmp['score'].idxmax()]\n",
    "        filename_csv = filename_prefix + 'top_scores_tfidf_' + score_type + '.csv'\n",
    "        filename_parm = filename_prefix + 'top_scores_tfidf_' + score_type + '.parm'\n",
    "        write_out_predictions(vectors_all,top_score_results_tmp, filename_csv, verbose)\n",
    "        top_score_results_tmp.to_csv(filename_parm, index=False)\n",
    "        print(\"Output written to %s\" %(filename_csv))\n",
    "        top_score_results_tmp\n",
    "\n",
    "    # Top Scores for TFIDF with NTLK preprocessed data\n",
    "    for score_type in score_types:\n",
    "        top_score_results_tmp = top_score_results[0:0]  # empty dataframe in each loop\n",
    "        for label in target_names:\n",
    "            df_tmp = result_df[(result_df['label'] == label) & (result_df['score_type'] == score_type) &\n",
    "                              (result_df['type'] == 'tfidf') & (result_df['preprocessor'] == 1)]\n",
    "            top_score_results_tmp.loc[top_score_results_tmp.shape[0]] = df_tmp.loc[df_tmp['score'].idxmax()]\n",
    "        filename_csv = filename_prefix + 'top_scores_tfidf_preproc_' + score_type + '.csv'\n",
    "        filename_parm = filename_prefix + 'top_scores_tfidf_preproc_' + score_type + '.parm'\n",
    "        write_out_predictions(vectors_all,top_score_results_tmp, filename_csv, verbose)\n",
    "        top_score_results_tmp.to_csv(filename_parm, index=False)\n",
    "        print(\"Output written to %s\" %(filename_csv))\n",
    "\n",
    "    sizes=[3000, 4000, 5000, 6000, 10000]\n",
    "    # Top Scores for TFIDF filtered by size\n",
    "    for size in sizes:\n",
    "        for score_type in score_types:\n",
    "            top_score_results_tmp = top_score_results[0:0]  # empty dataframe in each loop\n",
    "            for label in target_names:\n",
    "                df_tmp = result_df[(result_df['label'] == label) & (result_df['score_type'] == score_type) &\n",
    "                                  (result_df['type'] == 'tfidf') & (result_df['max_features'] == size)]\n",
    "                top_score_results_tmp.loc[top_score_results_tmp.shape[0]] = df_tmp.loc[df_tmp['score'].idxmax()]\n",
    "            filename_csv = filename_prefix + 'top_scores_tfidf_' + score_type + '_' + str(size) + '.csv'\n",
    "            filename_parm = filename_prefix + 'top_scores_tfidf_' + score_type + '_' + str(size) + '.parm'\n",
    "            write_out_predictions(vectors_all,top_score_results_tmp, filename_csv, verbose)\n",
    "            top_score_results_tmp.to_csv(filename_parm, index=False)\n",
    "            print(\"Output written to %s\" %(filename_csv))\n",
    "\n",
    "    # Top Scores for TFIDF with NTLK preprocessed data\n",
    "    for size in sizes:\n",
    "        for score_type in score_types:\n",
    "            top_score_results_tmp = top_score_results[0:0]  # empty dataframe in each loop\n",
    "            for label in target_names:\n",
    "                df_tmp = result_df[(result_df['label'] == label) & (result_df['score_type'] == score_type) &\n",
    "                                  (result_df['type'] == 'tfidf') & (result_df['preprocessor'] == 1) &\n",
    "                                  (result_df['max_features'] == size)]\n",
    "                top_score_results_tmp.loc[top_score_results_tmp.shape[0]] = df_tmp.loc[df_tmp['score'].idxmax()]\n",
    "            filename_csv = filename_prefix + 'top_scores_tfidf_preproc_' + score_type + '_' + str(size) + '.csv'\n",
    "            filename_parm = filename_prefix + 'top_scores_tfidf_preproc_' + score_type + '_' + str(size) + '.parm'\n",
    "            write_out_predictions(vectors_all,top_score_results_tmp, filename_csv, verbose)\n",
    "            top_score_results_tmp.to_csv(filename_parm, index=False)\n",
    "            print(\"Output written to %s\" %(filename_csv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "# To be used after a full run through\n",
    "#######################################################################################\n",
    "\n",
    "def read_create_classifiers(parm_file, train_data, preproc_train_data, train_labels):\n",
    "    \"\"\"Once the notebook has been run once and the results finalized this is the\n",
    "    only necessary function.  It can be used with any of the parm files written out in by the\n",
    "    write_out_results function to recreate the classifiers and return them for use.\n",
    "    \n",
    "    Args:\n",
    "        parm_file (string filename): a stored dataframe of parameters to create the classifier\n",
    "        train_data (unprocessed training data): the unprocessed training data\n",
    "        preproc_train_data: preprocessed training data (some parameters in call to vectorizer are\n",
    "            different when dealing with preprocessed data)\n",
    "        train_labels: a set of training labels that match the training data\n",
    "    Returns:\n",
    "        dictionary of classifiers: the index is the label of the classifier.  These are fitted classifiers\n",
    "            and can be used for predictions.\n",
    "    \"\"\"\n",
    "    input_parameters = pd.read_csv(parm_file)\n",
    "    return_classifiers = pd.DataFrame(columns=['label', 'classifier'])\n",
    "    return_classifiers = {}\n",
    "    \n",
    "#     vectorno,label,model,alpha,type,preprocessor,tokenizer,max_features,stop_words,lowercase,strip_accents,score_type,score\n",
    "    for index,row in input_parameters.iterrows():\n",
    "        if row['stop_words'] == None or row['stop_words'] is np.nan:\n",
    "            stop_words = None\n",
    "        else:\n",
    "            stop_words = row['stop_words']\n",
    "        if row['preprocessor'] == 1:\n",
    "            vect = TfidfVectorizer(tokenizer=identity, max_features=row['max_features'],\n",
    "                                   stop_words=stop_words,\n",
    "                                   lowercase=False,\n",
    "                                   strip_accents=None)\n",
    "            vect_train = vect.fit_transform(preproc_train_data)\n",
    "\n",
    "        else:\n",
    "            vect = tfidfVectorizer(tokenizer=identity, max_features=row['max_features'],\n",
    "                                   stop_words=stop_words,\n",
    "                                   strip_accents=row['strip_accents'],\n",
    "                                   lowercase=row['lowercase'])\n",
    "            vect_train = vect.fit_transform(train_data)\n",
    "\n",
    "        alpha_loc = ast.literal_eval(row['alpha']).get('alpha')\n",
    "        if row['model'] == 'bern':\n",
    "            nb_class = BernoulliNB(alpha=alpha_loc).fit(vect_train, train_labels[row['label']])\n",
    "        elif row['model'] == 'multi':\n",
    "            nb_class = MultinomialNB(alpha=alpha_loc).fit(vect_train, train_labels[row['label']])\n",
    "        # add the classifier with the labels to the return dataframe\n",
    "        #return_classifiers.loc[return_classifiers.shape[0]] = [row['label'],nb_class]\n",
    "        return_classifiers[row['label']] = nb_class\n",
    "    return return_classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19:28:32.479261: transforming training data with NLTK preprocessor\n",
      "19:41:00.173452: transforming dev data with NLTK preprocessor\n",
      "19:46:14.309278: completed NLTK preprocessor\n",
      "19:46:14.310868: Processing the next vector from base data, index 1\n",
      "19:46:28.044005: Processing the next vector from base data, index 2\n",
      "19:46:41.774401: Processing the next vector from base data, index 3\n",
      "19:46:56.332996: Processing the next vector from base data, index 4\n",
      "19:47:11.109611: Processing the next vector from base data, index 5\n",
      "19:47:26.406526: Processing the next vector from base data, index 6\n",
      "19:47:41.756125: Processing the next vector from base data, index 7\n",
      "19:47:54.912967: Processing the next vector from base data, index 8\n",
      "19:48:08.327294: Processing the next vector from base data, index 9\n",
      "19:48:22.463954: Processing the next vector from base data, index 10\n",
      "19:48:36.896665: Processing the next vector from base data, index 11\n",
      "19:48:51.550509: Processing the next vector from base data, index 12\n",
      "19:49:06.448355: Processing the next vector from base data, index 13\n",
      "19:49:19.638227: Processing the next vector from base data, index 14\n",
      "19:49:32.753446: Processing the next vector from base data, index 15\n",
      "19:49:47.003842: Processing the next vector from base data, index 16\n",
      "19:50:01.127077: Processing the next vector from base data, index 17\n",
      "19:50:15.908607: Processing the next vector from base data, index 18\n",
      "19:50:30.686963: Processing the next vector from base data, index 19\n",
      "19:50:43.506954: Processing the next vector from base data, index 20\n",
      "19:50:56.442042: Processing the next vector from base data, index 21\n",
      "19:51:10.255305: Processing the next vector from base data, index 22\n",
      "19:51:24.253676: Processing the next vector from base data, index 23\n",
      "19:51:38.720403: Processing the next vector from base data, index 24\n",
      "19:51:53.194665: Processing the next vector from base data, index 25\n",
      "19:52:06.373248: Processing the next vector from base data, index 26\n",
      "19:52:19.518441: Processing the next vector from base data, index 27\n",
      "19:52:33.735195: Processing the next vector from base data, index 28\n",
      "19:52:47.647474: Processing the next vector from base data, index 29\n",
      "19:53:02.425782: Processing the next vector from base data, index 30\n",
      "19:53:17.202532: Processing the next vector from base data, index 31\n",
      "19:53:30.078902: Processing the next vector from base data, index 32\n",
      "19:53:43.066525: Processing the next vector from base data, index 33\n",
      "19:53:56.923443: Processing the next vector from base data, index 34\n",
      "19:54:10.889558: Processing the next vector from base data, index 35\n",
      "19:54:25.315612: Processing the next vector from base data, index 36\n",
      "19:54:39.805687: Processing the next vector from base data, index 37\n",
      "19:54:53.044812: Processing the next vector from base data, index 38\n",
      "19:55:06.266425: Processing the next vector from base data, index 39\n",
      "19:55:20.491114: Processing the next vector from base data, index 40\n",
      "19:55:34.668326: Processing the next vector from base data, index 41\n",
      "19:55:49.474942: Processing the next vector from base data, index 42\n",
      "19:56:04.301937: Processing the next vector from base data, index 43\n",
      "19:56:17.150488: Processing the next vector from base data, index 44\n",
      "19:56:30.094259: Processing the next vector from base data, index 45\n",
      "19:56:44.070005: Processing the next vector from base data, index 46\n",
      "19:56:57.905527: Processing the next vector from base data, index 47\n",
      "19:57:12.324367: Processing the next vector from base data, index 48\n",
      "19:57:26.732593: Processing the next vector from base data, index 49\n",
      "19:57:40.005110: Processing the next vector from base data, index 50\n",
      "19:57:53.124335: Processing the next vector from base data, index 51\n",
      "19:58:07.392285: Processing the next vector from base data, index 52\n",
      "19:58:21.492202: Processing the next vector from base data, index 53\n",
      "19:58:36.232055: Processing the next vector from base data, index 54\n",
      "19:58:50.883606: Processing the next vector from base data, index 55\n",
      "19:59:03.692718: Processing the next vector from base data, index 56\n",
      "19:59:16.632857: Processing the next vector from base data, index 57\n",
      "19:59:30.405548: Processing the next vector from base data, index 58\n",
      "19:59:44.243849: Processing the next vector from base data, index 59\n",
      "19:59:58.618293: Processing the next vector from base data, index 60\n",
      "20:00:13.453992: Processing the next vector from base data, index 61\n",
      "20:00:26.679946: Processing the next vector from base data, index 62\n",
      "20:00:39.817506: Processing the next vector from base data, index 63\n",
      "20:00:53.889782: Processing the next vector from base data, index 64\n",
      "20:01:08.140621: Processing the next vector from base data, index 65\n",
      "20:01:23.085193: Processing the next vector from base data, index 66\n",
      "20:01:37.845932: Processing the next vector from base data, index 67\n",
      "20:01:50.705832: Processing the next vector from base data, index 68\n",
      "20:02:03.631695: Processing the next vector from base data, index 69\n",
      "20:02:17.490175: Processing the next vector from base data, index 70\n",
      "20:02:31.395651: Processing the next vector from base data, index 71\n",
      "20:02:45.725658: Processing the next vector from base data, index 72\n",
      "20:03:00.173605: Processing the next vector from preprocessed data, index 1\n",
      "20:03:05.362284: Processing the next vector from preprocessed data, index 2\n",
      "20:03:10.877733: Processing the next vector from preprocessed data, index 3\n",
      "20:03:15.733422: Processing the next vector from preprocessed data, index 4\n",
      "20:03:21.093450: Processing the next vector from preprocessed data, index 5\n",
      "20:03:25.992994: Processing the next vector from preprocessed data, index 6\n",
      "20:03:31.275032: Processing the next vector from preprocessed data, index 7\n",
      "20:03:36.111017: Processing the next vector from preprocessed data, index 8\n",
      "20:03:41.310040: Processing the next vector from preprocessed data, index 9\n",
      "20:03:46.203601: Processing the next vector from preprocessed data, index 10\n",
      "20:03:51.551579: Processing the next vector from preprocessed data, index 11\n",
      "20:03:56.360453: Processing the next vector from preprocessed data, index 12\n",
      "20:04:01.555378: Completed create_all_vectors\n",
      "20:04:01.563365: Starting calculate_score_all_models\n",
      "20:04:01.564249: checking row 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python2.7/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/lib64/python2.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:08:20.712151: checking row 1\n",
      "20:13:57.323610: checking row 2\n",
      "20:18:15.929765: checking row 3\n",
      "20:23:52.200339: checking row 4\n",
      "20:28:10.851198: checking row 5\n",
      "20:33:47.516424: checking row 6\n",
      "20:36:58.029656: checking row 7\n",
      "20:40:35.683043: checking row 8\n",
      "20:43:46.497611: checking row 9\n",
      "20:47:23.706024: checking row 10\n",
      "20:50:34.406519: checking row 11\n",
      "20:54:11.858759: checking row 12\n",
      "20:56:24.494422: checking row 13\n",
      "20:58:36.292511: checking row 14\n",
      "21:00:48.932721: checking row 15\n",
      "21:03:00.922222: checking row 16\n",
      "21:05:13.528664: checking row 17\n",
      "21:07:25.381345: checking row 18\n",
      "21:08:54.423134: checking row 19\n",
      "21:10:25.935328: checking row 20\n",
      "21:11:54.979336: checking row 21\n",
      "21:13:26.591130: checking row 22\n",
      "21:14:55.738207: checking row 23\n",
      "21:16:27.322204: checking row 24\n",
      "21:18:43.489297: checking row 25\n",
      "21:20:59.462087: checking row 26\n",
      "21:23:15.726494: checking row 27\n",
      "21:25:31.468703: checking row 28\n",
      "21:27:47.665754: checking row 29\n",
      "21:30:03.290140: checking row 30\n",
      "21:31:35.610025: checking row 31\n",
      "21:33:10.709176: checking row 32\n",
      "21:34:42.900471: checking row 33\n",
      "21:36:17.975822: checking row 34\n",
      "21:37:50.077384: checking row 35\n",
      "21:39:25.047162: checking row 36\n",
      "21:41:43.752818: checking row 37\n",
      "21:44:02.203236: checking row 38\n",
      "21:46:20.851086: checking row 39\n",
      "21:48:39.247114: checking row 40\n",
      "21:50:57.845935: checking row 41\n",
      "21:53:16.419653: checking row 42\n",
      "21:54:50.757420: checking row 43\n",
      "21:56:28.200368: checking row 44\n",
      "21:58:02.528363: checking row 45\n",
      "21:59:40.026918: checking row 46\n",
      "22:01:14.382473: checking row 47\n",
      "22:02:51.744021: checking row 48\n",
      "22:05:12.708885: checking row 49\n",
      "22:07:33.440677: checking row 50\n",
      "22:09:54.372381: checking row 51\n",
      "22:12:15.184092: checking row 52\n",
      "22:14:36.084793: checking row 53\n",
      "22:16:56.956502: checking row 54\n",
      "22:18:33.111621: checking row 55\n",
      "22:20:12.529925: checking row 56\n",
      "22:21:48.610739: checking row 57\n",
      "22:23:27.971828: checking row 58\n",
      "22:25:04.026588: checking row 59\n",
      "22:26:43.289483: checking row 60\n",
      "22:29:10.222863: checking row 61\n",
      "22:31:37.618040: checking row 62\n",
      "22:34:04.448053: checking row 63\n",
      "22:36:31.807899: checking row 64\n",
      "22:38:58.621871: checking row 65\n",
      "22:41:26.369505: checking row 66\n",
      "22:43:07.960003: checking row 67\n",
      "22:44:53.716292: checking row 68\n",
      "22:46:35.353753: checking row 69\n",
      "22:48:20.954672: checking row 70\n",
      "22:50:02.575943: checking row 71\n",
      "22:51:48.227149: checking row 72\n",
      "22:55:01.264849: checking row 73\n",
      "22:57:59.799082: checking row 74\n",
      "22:59:37.731346: checking row 75\n",
      "23:01:07.551092: checking row 76\n",
      "23:02:48.090696: checking row 77\n",
      "23:04:20.135910: checking row 78\n",
      "23:06:02.795656: checking row 79\n",
      "23:07:36.760876: checking row 80\n",
      "23:09:21.164806: checking row 81\n",
      "23:10:56.565251: checking row 82\n",
      "23:12:45.271706: checking row 83\n",
      "23:14:24.775464: finished calculate_score_all_models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:14:26.172047: Starting write_out_predictions to NB_predictions_top_scores_nofilter_precision.csv\n",
      "23:14:26.616365: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_nofilter_precision.csv\n",
      "23:14:28.227745: Starting write_out_predictions to NB_predictions_top_scores_nofilter_recall.csv\n",
      "23:14:28.631949: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_nofilter_recall.csv\n",
      "23:14:30.232677: Starting write_out_predictions to NB_predictions_top_scores_nofilter_roc_auc.csv\n",
      "23:14:30.579803: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_nofilter_roc_auc.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/site-packages/ipykernel_launcher.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:14:32.190522: Starting write_out_predictions to NB_predictions_top_scores_tfidf_precision.csv\n",
      "23:14:32.634026: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_precision.csv\n",
      "23:14:34.239227: Starting write_out_predictions to NB_predictions_top_scores_tfidf_recall.csv\n",
      "23:14:34.642737: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_recall.csv\n",
      "23:14:36.246106: Starting write_out_predictions to NB_predictions_top_scores_tfidf_roc_auc.csv\n",
      "23:14:36.592876: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_roc_auc.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/site-packages/ipykernel_launcher.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:14:38.207255: Starting write_out_predictions to NB_predictions_top_scores_tfidf_preproc_precision.csv\n",
      "23:14:38.525341: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_preproc_precision.csv\n",
      "23:14:40.134936: Starting write_out_predictions to NB_predictions_top_scores_tfidf_preproc_recall.csv\n",
      "23:14:40.511821: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_preproc_recall.csv\n",
      "23:14:42.120809: Starting write_out_predictions to NB_predictions_top_scores_tfidf_preproc_roc_auc.csv\n",
      "23:14:42.466789: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_preproc_roc_auc.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/site-packages/ipykernel_launcher.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:14:44.077492: Starting write_out_predictions to NB_predictions_top_scores_tfidf_precision_3000.csv\n",
      "23:14:44.335300: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_precision_3000.csv\n",
      "23:14:45.944104: Starting write_out_predictions to NB_predictions_top_scores_tfidf_recall_3000.csv\n",
      "23:14:46.333994: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_recall_3000.csv\n",
      "23:14:47.944812: Starting write_out_predictions to NB_predictions_top_scores_tfidf_roc_auc_3000.csv\n",
      "23:14:48.326525: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_roc_auc_3000.csv\n",
      "23:14:49.945189: Starting write_out_predictions to NB_predictions_top_scores_tfidf_precision_4000.csv\n",
      "23:14:50.234221: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_precision_4000.csv\n",
      "23:14:51.843286: Starting write_out_predictions to NB_predictions_top_scores_tfidf_recall_4000.csv\n",
      "23:14:52.238531: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_recall_4000.csv\n",
      "23:14:53.848026: Starting write_out_predictions to NB_predictions_top_scores_tfidf_roc_auc_4000.csv\n",
      "23:14:54.196687: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_roc_auc_4000.csv\n",
      "23:14:55.804945: Starting write_out_predictions to NB_predictions_top_scores_tfidf_precision_5000.csv\n",
      "23:14:56.073637: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_precision_5000.csv\n",
      "23:14:57.684773: Starting write_out_predictions to NB_predictions_top_scores_tfidf_recall_5000.csv\n",
      "23:14:58.082514: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_recall_5000.csv\n",
      "23:14:59.692091: Starting write_out_predictions to NB_predictions_top_scores_tfidf_roc_auc_5000.csv\n",
      "23:15:00.076063: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_roc_auc_5000.csv\n",
      "23:15:01.684427: Starting write_out_predictions to NB_predictions_top_scores_tfidf_precision_6000.csv\n",
      "23:15:01.954685: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_precision_6000.csv\n",
      "23:15:03.564952: Starting write_out_predictions to NB_predictions_top_scores_tfidf_recall_6000.csv\n",
      "23:15:03.970891: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_recall_6000.csv\n",
      "23:15:05.585991: Starting write_out_predictions to NB_predictions_top_scores_tfidf_roc_auc_6000.csv\n",
      "23:15:05.976541: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_roc_auc_6000.csv\n",
      "23:15:07.588696: Starting write_out_predictions to NB_predictions_top_scores_tfidf_precision_10000.csv\n",
      "23:15:07.869058: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_precision_10000.csv\n",
      "23:15:09.482691: Starting write_out_predictions to NB_predictions_top_scores_tfidf_recall_10000.csv\n",
      "23:15:09.930006: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_recall_10000.csv\n",
      "23:15:11.542149: Starting write_out_predictions to NB_predictions_top_scores_tfidf_roc_auc_10000.csv\n",
      "23:15:11.956292: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_roc_auc_10000.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/site-packages/ipykernel_launcher.py:88: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:15:13.579723: Starting write_out_predictions to NB_predictions_top_scores_tfidf_preproc_precision_3000.csv\n",
      "23:15:13.815960: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_preproc_precision_3000.csv\n",
      "23:15:15.435596: Starting write_out_predictions to NB_predictions_top_scores_tfidf_preproc_recall_3000.csv\n",
      "23:15:15.804047: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_preproc_recall_3000.csv\n",
      "23:15:17.433851: Starting write_out_predictions to NB_predictions_top_scores_tfidf_preproc_roc_auc_3000.csv\n",
      "23:15:17.782386: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_preproc_roc_auc_3000.csv\n",
      "23:15:19.408235: Starting write_out_predictions to NB_predictions_top_scores_tfidf_preproc_precision_4000.csv\n",
      "23:15:19.647390: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_preproc_precision_4000.csv\n",
      "23:15:21.272249: Starting write_out_predictions to NB_predictions_top_scores_tfidf_preproc_recall_4000.csv\n",
      "23:15:21.646351: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_preproc_recall_4000.csv\n",
      "23:15:23.274086: Starting write_out_predictions to NB_predictions_top_scores_tfidf_preproc_roc_auc_4000.csv\n",
      "23:15:23.621891: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_preproc_roc_auc_4000.csv\n",
      "23:15:25.233970: Starting write_out_predictions to NB_predictions_top_scores_tfidf_preproc_precision_5000.csv\n",
      "23:15:25.478498: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_preproc_precision_5000.csv\n",
      "23:15:27.101310: Starting write_out_predictions to NB_predictions_top_scores_tfidf_preproc_recall_5000.csv\n",
      "23:15:27.477873: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_preproc_recall_5000.csv\n",
      "23:15:29.093846: Starting write_out_predictions to NB_predictions_top_scores_tfidf_preproc_roc_auc_5000.csv\n",
      "23:15:29.477422: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_preproc_roc_auc_5000.csv\n",
      "23:15:31.093646: Starting write_out_predictions to NB_predictions_top_scores_tfidf_preproc_precision_6000.csv\n",
      "23:15:31.342783: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_preproc_precision_6000.csv\n",
      "23:15:32.969051: Starting write_out_predictions to NB_predictions_top_scores_tfidf_preproc_recall_6000.csv\n",
      "23:15:33.353816: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_preproc_recall_6000.csv\n",
      "23:15:34.970824: Starting write_out_predictions to NB_predictions_top_scores_tfidf_preproc_roc_auc_6000.csv\n",
      "23:15:35.366282: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_preproc_roc_auc_6000.csv\n",
      "23:15:36.992906: Starting write_out_predictions to NB_predictions_top_scores_tfidf_preproc_precision_10000.csv\n",
      "23:15:37.252227: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_preproc_precision_10000.csv\n",
      "23:15:38.875669: Starting write_out_predictions to NB_predictions_top_scores_tfidf_preproc_recall_10000.csv\n",
      "23:15:39.275602: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_preproc_recall_10000.csv\n",
      "23:15:40.887277: Starting write_out_predictions to NB_predictions_top_scores_tfidf_preproc_roc_auc_10000.csv\n",
      "23:15:41.272785: Predictions done, writing out\n",
      "Output written to NB_predictions_top_scores_tfidf_preproc_roc_auc_10000.csv\n",
      "23:15:41.519537: completed main\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# MAIN\n",
    "#############################################\n",
    "# This block does the following:\n",
    "# 1. Create the NLTK Preprocessed data\n",
    "# 2. Create the set of vectors with the unprocessed and preprocessed data\n",
    "# 3. Calculates the scores on all models\n",
    "\n",
    "# More progress printing when this is True\n",
    "verbose=True\n",
    "\n",
    "\n",
    "# Create the NLTK preprocessed data\n",
    "if verbose == True:\n",
    "    print('%s: transforming training data with NLTK preprocessor' %(str(datetime.datetime.now().time())))\n",
    "train_preproc_data = NLTKPreprocessor().fit(train_data).transform(train_data)\n",
    "if verbose == True:\n",
    "    print('%s: transforming dev data with NLTK preprocessor' %(str(datetime.datetime.now().time())))\n",
    "dev_preproc_data = NLTKPreprocessor().fit(dev_data).transform(dev_data)\n",
    "if verbose == True:\n",
    "    print('%s: completed NLTK preprocessor' %(str(datetime.datetime.now().time())))\n",
    "\n",
    "# Create the set of vectors:\n",
    "vectors_all = create_all_vectors(basetrain_data=train_data, basedev_data=dev_data,\n",
    "                        preprocessedtrain_data=train_preproc_data, preprocesseddev_data=dev_preproc_data,\n",
    "                        verbose=verbose)\n",
    "\n",
    "# calculate the scores for all the models\n",
    "result_df = calculate_score_all_models(vectors_all,verbose=verbose)\n",
    "\n",
    "# Write out all the results\n",
    "result_df.to_csv('all_results_' + str(datetime.date.today()) + \".csv\", index=False)\n",
    "\n",
    "# this stores the results of the predictions and parameters to a set of output files\n",
    "write_out_results(result_df)\n",
    "if verbose == True:\n",
    "    print('%s: completed main' %(str(datetime.datetime.now().time())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:15:41.534500: start test read_create_classifiers\n",
      "23:16:04.147927: completed test read_create_classifiers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'identity_hate': BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True),\n",
       " 'insult': BernoulliNB(alpha=10.0, binarize=0.0, class_prior=None, fit_prior=True),\n",
       " 'obscene': BernoulliNB(alpha=10.0, binarize=0.0, class_prior=None, fit_prior=True),\n",
       " 'severe_toxic': BernoulliNB(alpha=2.0, binarize=0.0, class_prior=None, fit_prior=True),\n",
       " 'threat': BernoulliNB(alpha=0.5, binarize=0.0, class_prior=None, fit_prior=True),\n",
       " 'toxic': BernoulliNB(alpha=15.0, binarize=0.0, class_prior=None, fit_prior=True)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A quick test that we can reread and create classifiers with the read_create_classifiers function\n",
    "if verbose == True:\n",
    "    print('%s: start test read_create_classifiers' %(str(datetime.datetime.now().time())))\n",
    "result_classifiers = read_create_classifiers('NB_predictions_top_scores_tfidf_preproc_roc_auc.parm', \n",
    "                                             train_data, train_preproc_data, train_labels)\n",
    "\n",
    "if verbose == True:\n",
    "    print('%s: completed test read_create_classifiers' %(str(datetime.datetime.now().time())))\n",
    "    \n",
    "result_classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Evaluation & Model Conclusion\n",
    "A number of things were tested for this model\n",
    "\n",
    "* A variety of different parameters and vectorizers\n",
    "  * Count and Tfidf vectors\n",
    "  * Variety of feature sizes\n",
    "  * Data preprocessed or not\n",
    "  * Removing stop words and accents\n",
    "* Calculation of three types of scoring, precision, recall and auc\n",
    "\n",
    "Lessons learned\n",
    "* Experience is key: being novice has made this process take much longer\n",
    "\n",
    "* Interpreters (I'm sure everyone knows this) are slow, and for the moment this is single threaded so even slower.  A side requiment of this is that hardware matters, particularly faster cores and plenty of memory.\n",
    "\n",
    "* While we can in some cases brute force the best parameters there are situations where unless we have time and a cluster for processing power, we must instead rely on educated guesswork and compromises.  The educated guesswork can obviously be helped by research and experience.\n",
    "\n",
    "* Gaussian Naive Bayes is not suitable for the very sparse inputs we are using so not testing these out.\n",
    "\n",
    "\n",
    "Results:  \n",
    "\n",
    "<TABLE>\n",
    "<TR><TH> label</TH><TH> model</TH><TH> alpha</TH><TH> type</TH><TH> preprocessor</TH><TH> tokenizer</TH><TH> max_features</TH><TH> stop_words</TH><TH> lowercase</TH><TH> strip_accents</TH><TH> score_type</TH><TH> score </TH></TR>\n",
    "<TR><TD> toxic</TD><TD> multi</TD><TD> 10</TD><TD> tfidf</TD><TD> 0</TD><TD> 0</TD><TD> None</TD><TD> None</TD><TD> TRUE</TD><TD> None</TD><TD> precision</TD><TD> 1 </TD></TR>\n",
    "<TR><TD> severe_toxic</TD><TD> multi</TD><TD> 10</TD><TD> tfidf</TD><TD> 0</TD><TD> 0</TD><TD> 6000</TD><TD> english</TD><TD> FALSE</TD><TD> None</TD><TD> precision</TD><TD> 0.8 </TD></TR>\n",
    "<TR><TD> obscene</TD><TD> multi</TD><TD> 2</TD><TD> tfidf</TD><TD> 0</TD><TD> 0</TD><TD> None</TD><TD> None</TD><TD> TRUE</TD><TD> None</TD><TD> precision</TD><TD> 1 </TD></TR>\n",
    "<TR><TD> threat</TD><TD> multi</TD><TD> 0.5</TD><TD> tfidf</TD><TD> 0</TD><TD> 0</TD><TD> None</TD><TD> None</TD><TD> FALSE</TD><TD> None</TD><TD> precision</TD><TD> 1 </TD></TR>\n",
    "<TR><TD> insult</TD><TD> multi</TD><TD> 2</TD><TD> tfidf</TD><TD> 0</TD><TD> 0</TD><TD> None</TD><TD> None</TD><TD> FALSE</TD><TD> None</TD><TD> precision</TD><TD> 1 </TD></TR>\n",
    "<TR><TD> identity_hate</TD><TD> multi</TD><TD> 2</TD><TD> tfidf</TD><TD> 1</TD><TD> 0</TD><TD> 3000</TD><TD> english</TD><TD> FALSE</TD><TD> None</TD><TD> precision</TD><TD> 0.884615 </TD></TR>\n",
    "<TR><TH> Average</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> 0.947435833 </TH></TR>\n",
    "</TABLE>\n",
    "\n",
    "<TABLE>\n",
    "<TR><TH> label</TH><TH> model</TH><TH> alpha</TH><TH> type</TH><TH> preprocessor</TH><TH> tokenizer</TH><TH> max_features</TH><TH> stop_words</TH><TH> lowercase</TH><TH> strip_accents</TH><TH> score_type</TH><TH> score </TH></TR>\n",
    "<TR><TD> toxic</TD><TD> bern</TD><TD> 1</TD><TD> tfidf</TD><TD> 0</TD><TD> 0</TD><TD> 10000</TD><TD> english</TD><TD> TRUE</TD><TD> unicode</TD><TD> recall</TD><TD> 0.886398 </TD></TR>\n",
    "<TR><TD> severe_toxic</TD><TD> bern</TD><TD> 0.5</TD><TD> tfidf</TD><TD> 0</TD><TD> 0</TD><TD> 10000</TD><TD> english</TD><TD> TRUE</TD><TD> None</TD><TD> recall</TD><TD> 0.953782 </TD></TR>\n",
    "<TR><TD> obscene</TD><TD> bern</TD><TD> 1</TD><TD> tfidf</TD><TD> 1</TD><TD> 0</TD><TD> 10000</TD><TD> english</TD><TD> FALSE</TD><TD> None</TD><TD> recall</TD><TD> 0.897975 </TD></TR>\n",
    "<TR><TD> threat</TD><TD> bern</TD><TD> 0.5</TD><TD> tfidf</TD><TD> 0</TD><TD> 0</TD><TD> 3000</TD><TD> None</TD><TD> TRUE</TD><TD> None</TD><TD> recall</TD><TD> 0.868421 </TD></TR>\n",
    "<TR><TD> insult</TD><TD> bern</TD><TD> 1</TD><TD> tfidf</TD><TD> 0</TD><TD> 0</TD><TD> 10000</TD><TD> english</TD><TD> TRUE</TD><TD> unicode</TD><TD>recall</TD><TD> 0.885738 </TD></TR>\n",
    "<TR><TD> identity_hate</TD><TD> bern</TD><TD> 0.5</TD><TD> tfidf</TD><TD> 1</TD><TD> 0</TD><TD> 4000</TD><TD> None</TD><TD> FALSE</TD><TD> None</TD><TD> recall</TD><TD> 0.88785 </TD></TR>\n",
    "<TR><TH> Average</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> 0.896694 </TH></TR>\n",
    "</TABLE>\n",
    "\n",
    "<TABLE>\n",
    "<TR><TH> label</TH><TH> model</TH><TH> alpha</TH><TH> type</TH><TH> preprocessor</TH><TH> tokenizer</TH><TH> max_features</TH><TH> stop_words</TH><TH> lowercase</TH><TH> strip_accents</TH><TH> score_type</TH><TH> score </TH></TR>\n",
    "<TR><TD> toxic</TD><TD> bern</TD><TD> 15</TD><TD> tfidf</TD><TD> 1</TD><TD> 0</TD><TD> 4000</TD><TD> english</TD><TD> FALSE</TD><TD> None</TD><TD> roc_auc</TD><TD> 0.858937 </TD></TR>\n",
    "<TR><TD> severe_toxic</TD><TD> bern</TD><TD> 2</TD><TD> tfidf</TD><TD> 1</TD><TD> 0</TD><TD> 4000</TD><TD> english</TD><TD> FALSE</TD><TD> None</TD><TD> roc_auc</TD><TD> 0.93949 </TD></TR>\n",
    "<TR><TD> obscene</TD><TD> bern</TD><TD> 10</TD><TD> tfidf</TD><TD> 1</TD><TD> 0</TD><TD> 4000</TD><TD> english</TD><TD> FALSE</TD><TD> None</TD><TD> roc_auc</TD><TD> 0.888569 </TD></TR>\n",
    "<TR><TD> threat</TD><TD> bern</TD><TD> 0.5</TD><TD> tfidf</TD><TD> 1</TD><TD> 0</TD><TD> 4000</TD><TD> None</TD><TD> FALSE</TD><TD> None</TD><TD> roc_auc</TD><TD> 0.899946 </TD></TR>\n",
    "<TR><TD> insult</TD><TD> bern</TD><TD> 10</TD><TD> tfidf</TD><TD> 1</TD><TD> 0</TD><TD> 3000</TD><TD> english</TD><TD> FALSE</TD><TD> None</TD><TD> roc_auc</TD><TD> 0.872004 </TD></TR>\n",
    "<TR><TD> identity_hate</TD><TD> bern</TD><TD> 1</TD><TD> tfidf</TD><TD> 1</TD><TD> 0</TD><TD> 4000</TD><TD> english</TD><TD> FALSE</TD><TD> None</TD><TD> roc_auc</TD><TD> 0.886601 </TD></TR>\n",
    "<TR><TH> Average</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> &nbsp;</TH><TH> 0.8909245 </TH></TR>\n",
    "</TABLE>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
