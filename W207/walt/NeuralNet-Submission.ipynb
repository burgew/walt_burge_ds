{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Pipeline: Toxic Language Classification \n",
    "**w207 Spring 2018 - Final Project Baseline**\n",
    "\n",
    "**Team: Paul, Walt, Yisang, Joe**\n",
    "\n",
    "\n",
    "\n",
    "### Project Description \n",
    "\n",
    "Our challenge is to build a multi-headed model thatâ€™s capable of detecting different types of of toxicity like threats, obscenity, insults, and identity-based hate.  The toxic language data set is sourced from Wikipedia and available as a public kaggle data set. \n",
    "\n",
    "Our goal is to use various machine learning techniques used in class to develop high quality ML models and pipelines.  \n",
    "\n",
    "1. Exercise and build upon concepts covered in class and test out at least 3 kinds of supervised models:\n",
    "    a. Regression (LASSO, Logistic)\n",
    "    b. Trees (RF, XGBoost)\n",
    "    c. DeepLearning (Tensorflow)\n",
    "2. Using stacking/ensembling methods for improving prediction metrics (K-Means, anomaly detection)\n",
    "3. Using unsupervised methods for feature engineering/selection\n",
    "\n",
    "For the baseline proposal, this file contains a first pass run through from data preprocessing to model evaluation using a regression model pipeline. \n",
    "\n",
    "https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import time\n",
    "import os.path\n",
    "import pickle\n",
    "\n",
    "#sklearn imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.cross_validation import StratifiedKFold, cross_val_score, train_test_split \n",
    "\n",
    "#NLTK imports\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import punkt as punkt\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# These imports enable the use of NLTKPreprocessor in an sklearn Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "#scipy imports\n",
    "from scipy.sparse import hstack\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "#Visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec \n",
    "import bokeh\n",
    "#! pip install bokeh\n",
    "\n",
    "#General imports\n",
    "import pprint\n",
    "\n",
    "# target classes\n",
    "target_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training observations: 111828\n",
      "training data shape: (111828L,)\n",
      "training label shape: (111828, 6)\n",
      "test data shape: (47743L,)\n",
      "test labels shape: (47743, 6)\n",
      "labels names: ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n"
     ]
    }
   ],
   "source": [
    "# read frames localy through csv\n",
    "train_df = pd.read_csv(\"../data/new_train.csv\")\n",
    "test_df = pd.read_csv(\"../data/new_test.csv\")\n",
    "\n",
    "np.random.seed(455)\n",
    "\n",
    "# Random index generator for splitting training data\n",
    "# Note: Each rerun of cell will create new splits.\n",
    "randIndexCut = np.random.rand(len(train_df)) < 0.7\n",
    "\n",
    "#Split up data\n",
    "train_data, train_labels = train_df[\"comment_text\"], train_df[target_names]\n",
    "test_data, test_labels = test_df[\"comment_text\"], test_df[target_names]\n",
    "\n",
    "print 'total training observations:', train_df.shape[0]\n",
    "print 'training data shape:', train_data.shape\n",
    "print 'training label shape:', train_labels.shape\n",
    "\n",
    "print 'test data shape:', test_data.shape\n",
    "print 'test labels shape:', test_labels.shape\n",
    "print 'labels names:', target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Processing\n",
    "\n",
    "The following cell contains the class based method and functions for text processing that reflects the final ETL pipeline for this case study.  \n",
    "\n",
    "This text process is the global text processing step for the final submission.  The processed data sets, prior to training, are saved to the data folder for other models to use in order to gain a more apples-to-apples comparision. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Joseph\n",
      "[nltk_data]     Lee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "class NLTKPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Text preprocessor using NLTK tokenization and Lemmatization\n",
    "\n",
    "    This class is to be used in an sklean Pipeline, prior to other processers like PCA/LSA/classification\n",
    "    Attributes:\n",
    "        lower: A boolean indicating whether text should be lowercased by preprocessor\n",
    "                default: True\n",
    "        strip: A boolean indicating whether text should be stripped of surrounding whitespace, underscores and '*'\n",
    "                default: True\n",
    "        stopwords: A set of words to be used as stop words and thus ignored during tokenization\n",
    "                default: built-in English stop words\n",
    "        punct: A set of punctuation characters that should be ignored\n",
    "                default: None\n",
    "        lemmatizer: An object that should be used to lemmatize tokens\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stopwords=None, punct=None,\n",
    "                 lower=True, strip=True):\n",
    "        \"\"\"Initialize method for NLTKPreprocessor instance\n",
    "\n",
    "        Simple initialization of specified instance variables:\n",
    "\n",
    "        Args:\n",
    "            self \n",
    "            stopwords: set of words to ignore as stop words, or a default set for English will be used\n",
    "            punct: set of punctuation characters to strip, or a default set will be used\n",
    "            lower: indicator of whether to convert all characters to lowercase, defaults to True\n",
    "            strip: indicator of whether to strip whitespace, defaults to True\n",
    "\n",
    "        Returns:\n",
    "            N/A: instance initializer\n",
    "        \"\"\"\n",
    "        \n",
    "        self.lower      = lower\n",
    "        self.strip      = strip\n",
    "        self.stopwords  = stopwords or set(sw.words('english'))\n",
    "        self.punct      = punct or set(string.punctuation)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit model with X and optional y\n",
    "\n",
    "        This function does nothing but return self, since as a processor in the sklearn Pipeline this preprocessor\n",
    "        has nothing analogous to \"fit\" logic. The tokenization logic is independent of specific dataset training, \n",
    "        and is fully realized in the transform() function. \n",
    "        This function exists as implementation of sklearn.BaseEstimator, for use in Pipeline.\n",
    "\n",
    "        Args:\n",
    "            self \n",
    "            X (array-like): independent variable\n",
    "            y (array-like): dependent variable\n",
    "            \n",
    "        Returns:\n",
    "            NLTKPreprocessor: self\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        \"\"\"Function exists as implementation of sklearn.BaseEstimator, for use in Pipeline.\n",
    "        This is simply for complying with interface.\n",
    "\n",
    "        Args:\n",
    "            self \n",
    "            X (array-like): input documents\n",
    "            \n",
    "        Returns:\n",
    "            string: joined documents\n",
    "        \"\"\"\n",
    "        return [\" \".join(doc) for doc in X]\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform input X to produce output to be processed by next element in sklearn Pipeline\n",
    "\n",
    "        This triggers the tokenization/lemmatization of the source documents.\n",
    "        This is invoked by the sklearn Pipeline.\n",
    "\n",
    "        Args:\n",
    "            self \n",
    "            X: input documents to be tokenized\n",
    "            \n",
    "        Returns:\n",
    "            list: tokenized documents reduced to simplest lemma form\n",
    "        \"\"\"\n",
    "        return [\n",
    "            list(self.tokenize(doc)) for doc in X\n",
    "        ]\n",
    "\n",
    "    \n",
    "    def tokenize(self, document):\n",
    "        \"\"\"Tokenize an input document, converting from a block of text into sentences, into tagged tokens,\n",
    "        generating a set of lemmas.\n",
    "\n",
    "        This method does the preprocessing work of sentence-based tokenization and then reduces words to lemmas\n",
    "\n",
    "        Args:\n",
    "            self \n",
    "            X (array-like): independent variable\n",
    "            y (array-like): dependent variable\n",
    "            \n",
    "        Returns:\n",
    "            Iterator[str]: an iterator over the tokens produced from the input documents\n",
    "        \"\"\"\n",
    "        # Break the document into sentences. This is necessary for part-of-speech tagging.\n",
    "        for sent in sent_tokenize(unicode(document,'utf-8')):\n",
    "\n",
    "            # Break the sentence into part of speech tagged tokens\n",
    "            for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "                # Apply preprocessing to the token\n",
    "                token = token.lower() if self.lower else token\n",
    "                token = token.strip() if self.strip else token\n",
    "                token = token.strip('_') if self.strip else token\n",
    "                token = token.strip('*') if self.strip else token\n",
    "\n",
    "                # If stopword, ignore token and continue\n",
    "                if token in self.stopwords:\n",
    "                    continue\n",
    "\n",
    "                # If punctuation, ignore token and continue\n",
    "                if all(char in self.punct for char in token):\n",
    "                    continue\n",
    "\n",
    "                # Lemmatize the token and yield\n",
    "                lemma = self.lemmatize(token, tag)\n",
    "                yield lemma\n",
    "\n",
    "                \n",
    "    def lemmatize(self, token, tag):\n",
    "        \"\"\"Convert a token into the appropriate lemma\n",
    "\n",
    "        Method uses the NLTK WordNetLemmatizer for part-of-speech tag-based lemmatization of words.\n",
    "\n",
    "        Args:\n",
    "            self \n",
    "            token: input word\n",
    "            tag: part-of-speech tag\n",
    "            \n",
    "        Returns:\n",
    "            string: lemma\n",
    "        \"\"\"\n",
    "        tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(tag[0], wn.NOUN)\n",
    "\n",
    "        return self.lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "def identity(arg):\n",
    "    \"\"\" Simple identity function works as a passthrough.\n",
    "\n",
    "        This function will be used with the Vectorizer classes, when tokenization will have been performed already.\n",
    "        In this scenario, the Vectorizer class will call this function in the place of its normal tokenization feature\n",
    "        and this function will simply return the input token.\n",
    "        \n",
    "        Args:\n",
    "            token (string): text token being evaluated by CountVectorizer or TfidfVectorizer\n",
    "            \n",
    "        Returns:\n",
    "            string: input token unchanged (processed earlier by NLTK) will tbe returned\n",
    "    \"\"\"\n",
    "    return arg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Text Preprocessing - training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "This block uses the NLTKPreprocessor to tokenize the input data and then the TfidfVectorizer to vectorize it. The NLTKPreprocessor will ignore English stop words and will lemmatize where possible. The vectorizer ignores words occuring in fewer than 5 documents, which sufficed to reduce the size of the words vector significantly. Also, the vectorizer will limit the total features (words) to 15000, prioritizing the most valuable ones with highest TF-IDF score.\n",
    "\n",
    "Note that in this case the tokenization available by default in TfidfVectorizer is disabled, since that is handled by the NLTKPreprocessor. This made it clear that tokenization is by far more expensive (time) than vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(455)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_vectors(train_data, test_data, max_features=None):\n",
    "    \"\"\"Preprocess (tokenize, vectorize) train and test datasets\n",
    "    \n",
    "    Datasets will be tokenized using NLTK and vectorized using TfidfVectorizer\n",
    "\n",
    "    Args:\n",
    "        train_data (array-like): training data to preprocess\n",
    "        test_data (array-like): test data to preprocess\n",
    "        max_features (int): maximum number of features to be produced by process\n",
    "\n",
    "    Returns:\n",
    "        N/A: instance initializer\n",
    "    \"\"\"\n",
    "\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "    # This preprocessor will be used to process data prior to vectorization\n",
    "    nltkPreprocessor = NLTKPreprocessor()\n",
    "    \n",
    "    # Note that this vectorizer is created with a passthru tokenizer(identity), no preprocessor and no lowercasing\n",
    "    # This is to account for the NLTKPreprocessor already taking care of tokenization.\n",
    "    tfidfVector = TfidfVectorizer(ngram_range=(1,2), min_df=5, max_df=.7, max_features=max_features,\n",
    "                                  tokenizer=identity, preprocessor=None, lowercase=False, stop_words={'english'})\n",
    "\n",
    "    print(\"\\nget_preprocessed_vectors() : Preprocessing input data, generating output of \"+ str(max_features) + \n",
    "          \" maximum features.\")\n",
    "\n",
    "    # Check if there is a serialized copy of the preprocessed training data, and if not the perform text preprocessing and\n",
    "    # save the serialized result for reuse.\n",
    "    pickle_file_name = 'train_preproc_data.pickle'\n",
    "    if (not os.path.exists(pickle_file_name)):\n",
    "        print \"Starting preprocessing of training data...\"\n",
    "        start_train_preproc = time.time()\n",
    "        nltkPreprocessor.fit(train_data)\n",
    "        train_preproc_data = nltkPreprocessor.transform(train_data)\n",
    "        finish_train_preproc = time.time()\n",
    "        print \"Completed tokenization/preprocessing of training data in {:.2f} seconds\".format(finish_train_preproc-start_train_preproc)\n",
    "    \n",
    "        with open(pickle_file_name,'w') as pickle_file:\n",
    "            pickle.dump(train_preproc_data,pickle_file)\n",
    "    else:\n",
    "        # If the serialized file already exists, simply load it for the next step of the process.\n",
    "        with open(pickle_file_name,'r') as pickle_file:\n",
    "            train_preproc_data = pickle.load(pickle_file)\n",
    "\n",
    "    tfidfVector.fit(train_preproc_data)\n",
    "        \n",
    "    # Check if there is a serialized copy of the vectorized counts, and if not regenerate the matrix and save the\n",
    "    # serialized result for reuse.        \n",
    "    pickle_file_name = 'train_tfidf_counts.'+str(max_features)+'.pickle'\n",
    "    if (not os.path.exists(pickle_file_name)):\n",
    "    \n",
    "        print \"Starting vectorization of training data...\"\n",
    "        start_train_vectors = time.time()\n",
    "        train_tfidf_counts = tfidfVector.transform(train_preproc_data)\n",
    "        finish_train_vectors = time.time()\n",
    "        print \"Completed vectorization of training data in {:.2f} seconds\".format(finish_train_vectors-start_train_vectors)\n",
    "    \n",
    "        with open(pickle_file_name,'w') as pickle_file:\n",
    "            pickle.dump(train_tfidf_counts,pickle_file)\n",
    "    else:\n",
    "        # If the serialized file already exists, simply load it for the next step of the process.\n",
    "        with open(pickle_file_name,'r') as pickle_file:\n",
    "            train_tfidf_counts = pickle.load(pickle_file)\n",
    "    \n",
    "    # Check if there is a serialized copy of the preprocessed test data, and if not the perform text preprocessing and\n",
    "    # save the serialized result for reuse.\n",
    "    pickle_file_name = 'test_preproc_data.pickle'\n",
    "    if (not os.path.exists(pickle_file_name)):\n",
    "        print \"\\nStarting preprocessing of  data...\"\n",
    "        start_test_preproc = time.time()\n",
    "        nltkPreprocessor.fit(test_data)\n",
    "        test_preproc_data = nltkPreprocessor.transform(test_data)\n",
    "        finish_test_preproc = time.time()\n",
    "        print \"Completed tokenization/preprocessing of test data in {:.2f} seconds\".format(finish_test_preproc-start_test_preproc)\n",
    "\n",
    "        with open(pickle_file_name,'w') as pickle_file:\n",
    "            pickle.dump(test_preproc_data,pickle_file)\n",
    "    else:\n",
    "        # If the serialized file already exists, simply load it for the next step of the process.\n",
    "        with open(pickle_file_name,'r') as pickle_file:\n",
    "            test_preproc_data = pickle.load(pickle_file)\n",
    "    \n",
    "    pickle_file_name = 'test_tfidf_counts.'+str(max_features)+'.pickle'\n",
    "    if (not os.path.exists(pickle_file_name)):\n",
    "\n",
    "        print \"Starting vectorization of test data...\"\n",
    "        start_test_vectors = time.time()\n",
    "        test_tfidf_counts = tfidfVector.transform(test_preproc_data)\n",
    "        finish_test_vectors = time.time()\n",
    "        print \"Completed vectorization of test data in {:.2f} seconds\".format(finish_test_vectors-start_test_vectors)\n",
    "        \n",
    "        print(\"\\nVocabulary (tfidf) size is: {}\").format(len(tfidfVector.vocabulary_))\n",
    "        vocab_entries = {k: tfidfVector.vocabulary_[k] for k in tfidfVector.vocabulary_.keys()}\n",
    "        vocab_entries = pd.Series(vocab_entries).to_frame()\n",
    "        vocab_entries.columns = ['count']\n",
    "        vocab_entries = vocab_entries.sort_values(by='count')\n",
    "\n",
    "        print(\"Sample vocabulary from TfidfVectorizer:\")\n",
    "        print(pp.pprint(vocab_entries.head(10)))\n",
    "        print(\"...\")\n",
    "        print(pp.pprint(vocab_entries.tail(10)))\n",
    "        print(\"Number of nonzero entries in matrix: {}\").format(train_tfidf_counts.nnz)\n",
    "\n",
    "        with open(pickle_file_name,'w') as pickle_file:\n",
    "            pickle.dump(test_tfidf_counts,pickle_file)\n",
    "    else:\n",
    "        # If the serialized file already exists, simply load it for the next step of the process.\n",
    "        with open(pickle_file_name,'r') as pickle_file:\n",
    "            test_tfidf_counts = pickle.load(pickle_file)\n",
    "\n",
    "    # Print sample column wise sum, we can see that an observation can have multiple classes.\n",
    "    count_df = pd.DataFrame(train_labels.apply(np.sum,1), columns = [\"counts\"])\n",
    "    count_df = count_df[((count_df[\"counts\"] >= 1))]\n",
    "    count_df.head(10)\n",
    "    \n",
    "    return train_tfidf_counts, test_tfidf_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final MLPClassifier Training and Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification with Neural Net (sklearn.MLPClassifier)\n",
    "In choosing a neural net model for text classification, the output layer should have the same number of nodes as the number of classification labels. In this case, there are 6 labels and as such not only will the output layer have 6 nodes, but the final hidden layer as well. The input layer will have the same number of nodes as features, normally, and ideally the initial hidden layer will be between that and the number of classes.\n",
    "\n",
    "In this case, we're limiting our feature set to 5,000 principal components, and it was not possible to use a number of initial hidden layer nodes at all close to that, running this process on a Macbook. So, setting the initial hidden layer to 12 gave at least some benefit of being less than the number of features and greater than the number of output classes. This (12,6) model is the one that ended up producing best (most accurate) results.\n",
    "\n",
    "Note that, nod toward deeper learning, a (10,8,6) model was also tested, but this ended up demonstrating overfitting, with a signficantly higher accuracy score on test data than on dev data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClassifier():\n",
    "    \"\"\"Get an MLPClassifier configured as per the results of best parameter analysis\n",
    "    This classifier will be used by testing logic in this notebook.\n",
    "\n",
    "    Args:\n",
    "        N/A \n",
    "            \n",
    "    Returns:\n",
    "        MLPClassifier: configured classifier for testing\n",
    "    \"\"\"\n",
    "    return MLPClassifier(hidden_layer_sizes=(12,6), solver='adam', early_stopping=False, activation='relu',\n",
    "                         tol=1e-13, alpha=1, learning_rate='adaptive', learning_rate_init=0.01 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roc_auc(classifier, train_roc_data, train_roc_labels, test_roc_data, test_roc_labels):\n",
    "    \"\"\"Get ROC AUC scores for multi-label data by binarizing labels\n",
    "\n",
    "    Args:\n",
    "        classifier (sklearn classifier): model for which to report ROC AUC scores\n",
    "        train_roc_data (array-like): array of training data\n",
    "        train_roc_labels (array-like): array of labels assiociated with training data\n",
    "        test_roc_data (array-like): array of test data\n",
    "        test_roc_labels (array-like): array of labels assiociated with test data\n",
    "            \n",
    "    Returns:\n",
    "        list: per-label ROC AUC scores\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fitting again with binarized labels and predicting again to support per-label roc_auc scores\n",
    "    binarized_train_labels = label_binarize(train_roc_labels, classes=[0, 1, 2, 3, 4, 5])\n",
    "    binarized_test_labels = label_binarize(test_roc_labels, classes=[0, 1, 2, 3, 4, 5])\n",
    "    \n",
    "    print \"\\nget_roc_auc(): train_roc_data size {}, test_roc_data size {}\".format(train_roc_data.shape,\n",
    "                                                                                      test_roc_data.shape)\n",
    "\n",
    "    # While this is multilabel data, the sklearn ROC AUC scoring feature doesn't support multilabel data directly.\n",
    "    # So, instead the model will be re-trained with binarized training labels and the predicted probabilities used\n",
    "    # To derive ROC AUC for each label. This is mainly for comparison with other models, since these numbers won't\n",
    "    # be directly related to the multilabel classification.\n",
    "    print(\"Re-fitting and scoring for per-label roc_auc scores...\")\n",
    "    y_score = classifier.fit(train_roc_data, binarized_train_labels).predict_proba(test_roc_data)\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = []\n",
    "    for ind, label in enumerate(target_names):\n",
    "        fpr[ind], tpr[ind], _ = metrics.roc_curve(binarized_test_labels[:, ind], y_score[:, ind])\n",
    "        roc_auc.append(metrics.auc(fpr[ind], tpr[ind]))\n",
    "\n",
    "    print \"pre-label ROC AUC scores: \", roc_auc\n",
    "    return roc_auc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_overfitting_MLP(classifier, alpha=.1):\n",
    "    \"\"\"Evaluate a classifier and return the significance of the difference between limited scoring and full scoring\n",
    "    This significance is calculated using a paired t-test\n",
    "\n",
    "    Args:\n",
    "        classifier (sklearn classifier): Pre-configured classifier to be evaluated for over/underfitting.\n",
    "        alpha (float): error value as threshold for significance\n",
    "            \n",
    "    Returns:\n",
    "        float: t-test statistic\n",
    "        float: p-value\n",
    "    \"\"\"    \n",
    "    \n",
    "    # First take a 30% split of input data for limited focus\n",
    "    X_limited, X_ignore, y_limited, y_ignore = train_test_split(train_tfidf_counts, train_labels, \n",
    "                                                                test_size=0.3, random_state=455)\n",
    "    print \"test_overfitting_MLP(): Shape of focused data and remaining data: \", X_limited.shape, X_ignore.shape\n",
    "    \n",
    "    # Then, take a 80%/20% train/test split of the limited focus data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_limited, y_limited, test_size=0.2, random_state=455)\n",
    "    print \"Shape of limited train and test data for focused testing: \",X_train.shape, y_train.shape, X_test.shape\n",
    "    \n",
    "    # Perform cross-validation with the focused train data subset\n",
    "    train_scores = cross_val_score(classifier, X_train, y_train,\n",
    "                             scoring=\"neg_mean_squared_error\", cv=10)\n",
    "    \n",
    "\n",
    "    # deal with any nans and negative values, get mean for simple comparison\n",
    "    train_scores = np.nan_to_num(train_scores)\n",
    "    train_scores = np.absolute(train_scores)\n",
    "    mean_train_score = np.mean(train_scores)\n",
    "    \n",
    "    print \"train mean squared error score: {:.2f}\".format(mean_train_score)\n",
    "\n",
    "    # Perform cross-validation with the full post-preprocessed train_data\n",
    "    test_scores = cross_val_score(classifier, train_tfidf_counts, train_labels,\n",
    "                             scoring=\"neg_mean_squared_error\", cv=10)    \n",
    "    \n",
    "    # deal with any nans and negative values, get mean for simple comparison\n",
    "    test_scores = np.nan_to_num(test_scores)\n",
    "    test_scores = np.absolute(test_scores)\n",
    "    mean_test_score = np.mean(test_scores)\n",
    "    \n",
    "    print \"test mean squared error score: {:.2f}\".format(mean_test_score)\n",
    "    \n",
    "    if mean_test_score < mean_train_score:\n",
    "        error_case = \"underfitting\"\n",
    "    else:\n",
    "        error_case = \"overfitting\"\n",
    "    \n",
    "    t_stat = ttest_ind(train_scores, test_scores)\n",
    "    print \"t test statistic for both per-label score sets: \", t_stat\n",
    "    \n",
    "    \n",
    "    if t_stat.pvalue < alpha:\n",
    "        print \"p-value {:.2f} <= alpha {:.2f}, potentially indicating {}.\".format(t_stat.pvalue, alpha, error_case)\n",
    "            \n",
    "    return t_stat\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_MLP_scores(train_input_data, train_input_labels, test_input_data, test_inputlabels):  \n",
    "    \n",
    "    print \"\\ngenerate_MLP_scores()\"\n",
    "    # This MLPClassifier will be fit using training data and subsequently used to predict labels using test\n",
    "    # data, for scoring.\n",
    "    classifier = getClassifier()\n",
    "\n",
    "    # Fit using the training data and time the process for reference\n",
    "    full_train_start = time.time()\n",
    "    classifier.fit(train_input_data, train_labels)\n",
    "    full_train_stop = time.time()\n",
    "\n",
    "    duration = (full_train_stop-full_train_start)/60\n",
    "    print('Fitting train data completed, after {:.2f} minutes.'.format(duration))\n",
    "\n",
    "    # Generate predictions using the test TFIDF data and collect a series of scores\n",
    "    test_pred = classifier.predict(test_input_data)\n",
    "    acc_score = metrics.accuracy_score(test_labels, test_pred)\n",
    "\n",
    "    # Note that, since this is multilabel data, an F1 score must be evaluated with either results weighted across labels or\n",
    "    # as samples taken from each.\n",
    "    precision_recall_fscore = metrics.precision_recall_fscore_support(test_labels, test_pred, average=None)\n",
    "    precision = metrics.precision_score(test_labels, test_pred, average=None)\n",
    "    recall = metrics.recall_score(test_labels, test_pred, average=None)\n",
    "\n",
    "    # Prediction probabilities will be saved for comparison with other models and processing by ensembles\n",
    "    predict_probs = classifier.predict_proba(test_input_data)\n",
    "\n",
    "    print(\"Accuracy score from test predict: {}\".format(acc_score))\n",
    "    print(\"Precision score from test predict: {}\".format(precision))\n",
    "    print(\"Recall score from test predict: {}\".format(recall))\n",
    "\n",
    "    roc_auc = get_roc_auc(classifier, train_input_data, train_input_labels, test_input_data, test_input_labels)\n",
    "\n",
    "    print \"ROC AUC score from test predict: \", roc_auc\n",
    "\n",
    "    # In order to Save the complete collection of scores, a pandas.DataFrame will be created and used to create\n",
    "    # \"scoring.csv\".\n",
    "    scoring_arr = np.asarray(precision_recall_fscore)\n",
    "    scoring_arr = np.vstack([scoring_arr,roc_auc])\n",
    "    scoring_submission = pd.DataFrame(data=scoring_arr, columns=target_names, index=['precision', 'recall', \n",
    "                                                                                     'fbeta_score', 'support',\n",
    "                                                                                     'roc_auc'])\n",
    "    print(\"Precision, recall, fbeta_score, support and ROC AUC:\")\n",
    "    print(scoring_submission)\n",
    "    scoring_submission.to_csv(\"../data/NN.scoring.\"+str(max_features)+\".csv\")\n",
    "\n",
    "    # The predicted probabilities from the initial version of the model will be saved in CSV file \"submission.csv\"\n",
    "    prediction_submission = pd.DataFrame(data=predict_probs,columns=target_names)\n",
    "    print(prediction_submission[0:10]) # print frame output \n",
    "    prediction_submission.to_csv(\"../data/NN.submission.\"+str(max_features)+\".csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final LSA Feature Selection - training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA/LSA\n",
    "Principal Component Analysis (PCA) and Latent Semantic Analysis (LSA) are both operations that use Singular Value Decomposition to reduce the dimensionality of a dataset. PCA is applied to a term-covariance matrix, whereas LSA is applied to a term-document matrix. As such, LSA is appropriate for machine learning algorithms using scikit-learn TfidfVectorizer. Additionally PCA, as implemented in scikit-learn, cannot handle the sparse matrices that are produced by such vectorization tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of principal components to identify for use in classification processes\n",
    "target_components = 4000\n",
    "\n",
    "def get_LSA(target_components, train_data, test_data):\n",
    "\n",
    "    print \"\"\n",
    "    # Check if there is a serialized copy of the Principal Components data for the training dataset, and if not then\n",
    "    # perform LSA processing and save the serialized result for reuse.\n",
    "    pickle_file_name = 'lsa_train_counts.'+max_features+'.pickle'\n",
    "    if (not os.path.exists(pickle_file_name)):\n",
    "        svd = TruncatedSVD(n_components=target_components, algorithm='arpack')\n",
    "        print \"Starting LSA on train counts with {} components...\".format(target_components)\n",
    "        train_start=time.time()\n",
    "        lsa_train_counts = svd.fit_transform(train_tfidf_counts)\n",
    "        train_stop=time.time()\n",
    "        print \"Train counts transform took {:.2f} minutes.\".format((train_stop-train_start)/60)\n",
    "    \n",
    "        with open(pickle_file_name,'w') as pickle_file:\n",
    "            pickle.dump(lsa_train_counts,pickle_file)\n",
    "    else:\n",
    "        # If the serialized file already exists, simply load it for the next step of the process.\n",
    "        with open(pickle_file_name,'r') as pickle_file:\n",
    "            lsa_train_counts = pickle.load(pickle_file)\n",
    "\n",
    "\n",
    "    # Check if there is a serialized copy of the Principal Components data for the test dataset, and if not then\n",
    "    # perform LSA processing and save the serialized result for reuse.\n",
    "    pickle_file_name = 'lsa_test_counts.'+max_features+'pickle'\n",
    "    if (not os.path.exists(pickle_file_name)):\n",
    "        print \"Starting LSA on dtest counts with {} components...\".format(target_components)\n",
    "        test_start=time.time()\n",
    "        lsa_test_counts = svd.fit_transform(test_tfidf_counts)\n",
    "        test_stop=time.time()\n",
    "        print \"Test counts transform took {:.2f} minutes.\".format((test_stop-test_start)/60)\n",
    "    \n",
    "        with open(pickle_file_name,'w') as pickle_file:\n",
    "            pickle.dump(lsa_test_counts,pickle_file)\n",
    "    else:\n",
    "        # If the serialized file already exists, simply load it for the next step of the process.\n",
    "        with open(pickle_file_name,'r') as pickle_file:\n",
    "            lsa_test_counts = pickle.load(pickle_file) \n",
    "            \n",
    "    return lsa_train_counts, lsa_test_counts\n",
    "        \n",
    "#lsa_train_counts, lsa_test_counts = get_LSA(target_components, train_tfidf_counts, test_tfidf_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Overfit Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# The following logic will iterate through a cycle of preprocessing input data for a specific set of maximum features,\n",
    "# then test the MLPClassifier with the preprocessed data and produce score and probability output as CSV files in\n",
    "# ../data. The maximum feature values to be tested will include 3000, 4000, 5000, 6000, 10000, and unlimited (None).\n",
    "#\n",
    "# After this loop is complete, a test will be executed to determine whether there is overfitting with the MLPClassifier.\n",
    "\n",
    "for max_features in [3000, 4000, 5000, 6000, 10000, None]:\n",
    "    train_tfidf_counts, test_tfidf_counts = get_preprocessed_vectors(train_data, test_data, \n",
    "                                                                     max_features=max_features)\n",
    "    generate_MLP_scores(train_tfidf_counts, train_labels, test_tfidf_counts, test_labels)\n",
    "    \n",
    "t_stat = test_overfitting_MLP(getClassifier(), alpha=.1)\n",
    "\n",
    "print \"classifier overfitting t-test result: \"\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Conclusion \n",
    "\n",
    "Overall, Neural Networks show signficant promise for classifying toxic language.  Both train and test sets show high average performance scores across AUC scores that are similar in range (we can assume overfit it relatively minimal).\n",
    "\n",
    "However, despite high AUC scores, the recall is decent but the precision is fairly low in comparison (~0.50 Avg. Precision), meaning that this model has high number of False Positives and may be struggling with the subsetted intererestions between the target classes,  \n",
    "\n",
    "It is interesting to note that with there were signs of overfit with using LSA methods as shown by the t-test function in the cell above shows signficant evidence of overfit.  The compression through LSA may have removed some variance that are critical for classifying between certain target classes.\n",
    "\n",
    "Another interesting source of clear overfitting was from deepening the architecture and incorporating an additional layer, which led to deviations between train and test scores to grow in magnitude.  The additional of an additional layer likely increased the models fit to noise unique to the train set but not seen in the test set. \n",
    "\n",
    "The best model that we came up with given the time constraint had the node architecture of (12,6) and using the `adam` solver and `relu` activation function.  \n",
    "\n",
    "Rectifer's like `relu` gave performance lift compared to `sigmoid` activation functions for this case study.   Rectifiers can often speed up training and due to it's one-sided nature can be optimal for sparse data sets compared to `sigmoids` that are two-sided and conforms everything on a probability scale.  \n",
    "\n",
    "Additionally, n-grams between 1 and 2 seem optimal for this case, while any n > 3 decreases performance. This is interesting because it may mean that future reserach should consider using n-chars.  This would explode the feature space but with deeplearning and Convolutional architecture, it may offer additional performance lift.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
